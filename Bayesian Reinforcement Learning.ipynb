{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Authors: Toke~ Faurby~ (s136232)~ and~ Maciej~ Korzepa~ (mjko).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important notes\n",
    " * illustrate the knowledge of the Bayesian workflow.\n",
    " * Structure: The notebook presents a clear cohesive data analysis story, which is enjoyable to read\n",
    " * Statistical terms: Statistical terms are used accurately and with clarity\n",
    "\n",
    "#### TODO:\n",
    " * Discussion of problems, and potential improvements\n",
    " * Conclusion:\n",
    "     * The main conclusion of the data analysis should be clear\n",
    "     \n",
    " * Convergence diagnostics (Rhat, divergences, neff)\n",
    " * Model comparison if applicable (e.g. with loo)\n",
    " * Posterior predictive checking\n",
    " * Predictive performance assessment if applicable (e.g. classification accuracy)\n",
    " * Potentially sensitivity analysis\n",
    "\n",
    "\n",
    "\n",
    "#### DONE: \n",
    " * Introduction: \n",
    "     * The introduction is inviting, presents an overview of the notebook. Information is relevant and presented in a logical order.\n",
    " * Description of the data\n",
    " * Description of analysis problem\n",
    " * Description of Frequentis UCB\n",
    " * Description of Bayesian UCB\n",
    " * Description of Thompson Sampling\n",
    " * Description of Hierarchical extensions\n",
    " * Description of the prior\n",
    " * Inlude code\n",
    "\n",
    "\n",
    "Things we skip\n",
    " * Stan Code \n",
    " * How Stan model is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Reinforcement learning** is about goal directed learning from interactions, and is generally thought of as distinct from supervised and unsupervised learning.\n",
    "The goal is to learn what actions to take, and when to take them, so as to optimize long-term performance.\n",
    "This may involve sacrificing immediate reward to obtain greater reward in the long-term or just to obtain more information about the environment.\n",
    "The tradeoff between maximizing reward and learning about the environment is called the _exploration/exploitation dillemma_, and it is one of the problems at the core of reinforcement learning.\n",
    "\n",
    "\n",
    "**Bayesian methods** uses Bayes rule to define a posterior distribution, based on a prior on the model parameters and the likelihood of the obsevations given model parameters.\n",
    "Should we obtain even more observations, the old posterior becomes the new prior and the process is repeated.\n",
    "This property makes Bayesian models attractive for reinforcement learning, as the ability to effectively utilize new observations as they become available is important for overall performance (unlike many other data analysis problems).\n",
    "This explicit modelling of probabilities gives rise to principled methods for incorporating prior information and action-selection (exploration/exploitation) as a function of the uncertainty in learning.\n",
    "Bayesian methods also enable hierarchical approaches, that enhace data efficiency even further.\n",
    "\n",
    "\n",
    "In this notebook we will show the principles of **Bayesain reinforcement learning** using the classic and very simple problem - the **multi-armed bandit problem**.\n",
    "At each time step the agent must select one of $K$ arms, and will subsequently recieve a reward based on some unknown probability distribution $p(\\theta_k)$.\n",
    "The goal is to get as high a total reward as possible.\n",
    "The performance is often measured in terms of expected regret, i.e. the difference between the selected actions, and the optimal action.\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left(Regret(T)\\right) \n",
    "=\n",
    "\\mathbb{E} \\left[ \\sum_{t=1}^T \\left( r(a^*) - r(a_t) \\right)\\right]\n",
    "$$\n",
    "\n",
    "where $r(a)$ is the recieved reward after performing action $a$. \n",
    "$a^*$ is the action that gives the highest expected reward.\n",
    "\n",
    "The environment is therefore static, greatly simplifying the problem, but many of the general properties still hold.\n",
    "\n",
    "\n",
    "## Content\n",
    "1. The Environment: Multi-Armed Bernoulli Bandits\n",
    "* The Agents - Frequentist baseline, Bayesian approach, and hierarchical Bayesian approach\n",
    "* Implementation - what we did and how it works\n",
    "* Results\n",
    "* Conclusion and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Environment: Multi-Armed Bandits (K-MAB)\n",
    "\n",
    "The **data** is obtained iteratively through interacting with the environment, which we define as follows:\n",
    "\n",
    "When initialized $K$ parameters $\\theta_k$ are sampled from a $Beta(\\alpha, \\beta)$ distribution, with $\\alpha=10$ and $\\beta=40$ (these values are arbitrary, but we don't want to use a uniform distribution, as it would then match our prior).\n",
    "One episode lasts $1000$ steps, and at each time step, $t$ the agent picks an action, $a_t$, and recieves a reward, sampled from a Bernoulli distribution, with parameter $\\theta_{a_t}$.\n",
    "\n",
    "The agent must therefore balance selecting arms that it believes are good (exploit high expected $\\theta_k$) and unknown arms that might be even better (explore uncertain $\\theta_k$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NB:** Two files `agents.py` and `bandit.py` hold the code describing the agents and bandit environment respectively. The code is reproduced at the end of the notebook (in case you want to run it, run those first). The code is also available [here](https://github.com/Faur/Multi-Armed-Bayesian-Bandits). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from bandit import *\n",
    "except:\n",
    "    print('Download the code from github: https://github.com/Faur/Multi-Armed-Bayesian-Bandits')\n",
    "    print('or run the code block in the appendix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agents\n",
    "In order to demonstrate the effectiveness of Bayesian approaches we use the popular frequentist *upper confidence bound* (UCB) method as a benchmark.\n",
    "For Bayesian approaches we have selected '*Bayesian UCB*', for easy direct comparison, and '*Thompson Sampling*' (aka. probability matching), which achieves state of the art performance on the $k=2$ task.\n",
    "We also examin the hierarchical extensions of the two Bayesian approaches.\n",
    "\n",
    "\n",
    "## Frequentist UCB\n",
    "The idea behind frequentist UCB is that we should always explore to some extent, as we can never be certain whether or not we have foun the optimal arms.\n",
    "Exploratory actions should be selected based on their potential for being optimal, taking into account both how close their estimates are to being optimal and the uncertainties in those estimates.\n",
    "This is done through optimistic action selection, using the following selection criteria:\n",
    "\n",
    "$$\n",
    "a_t = \\arg \\max_a \\left[ \\overline{r(a)} + c\\sqrt{\\frac{\\ln{t}}{N(a)}} \\right]\n",
    "$$\n",
    "\n",
    "Where $\\overline{r(a)}$ is the emperical mean reward recieved from performing action $a$, $c$ is a hyperparameter determining the confidence level, and $N(a)$ returns the number of times action $a$ has been selected.\n",
    "\n",
    "Each time $a$ is selected the associated is reduced ($N(a)$ increases).\n",
    "Similarly when an action other than $a$ is selected  the uncertainty estimate increases ($t$ increases)\n",
    "The natural logarithm insures that the means that this increase in uncertainty gets smaller over time, but it is still unbounded, meaning that in the limit all actions will be taken infinitely many times (constant exploration).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian Approach\n",
    "According to K-MAB model, the only unknown quantities are action dependent outcome probabilities $P(\\cdot | a)$. \n",
    "We can learn these probabilities using Bayesian interference during sequential interaction with the MAB.\n",
    "For our Bernoulli K-MAB model, action outcome probabilities are parametrized by vector $\\mathbf{\\theta} \\in [0,1]^{K}$.\n",
    "As each interaction with the model corresponds to performing a Bernoulli trial, the natural choice of prior for $\\mathbf{\\theta}$ is Beta distribution for each arm $a$ i.e. \n",
    "\n",
    "$$p(\\theta_a) \\sim Beta(\\alpha_a,\\beta_a)$$\n",
    "\n",
    "The prior for $\\theta_a$ is chosen to be a non-informative prior $Beta(1,1)$, as we don't have any prior knowledge about possible values of $\\theta_a$.\n",
    "As this is also conjugate prior for bernoulli distribution, we can easily calculate the posterior of $\\theta_a$ given one observation:\n",
    "\n",
    "$$p(\\theta_a|y) \\propto Beta(\\alpha_a+y,\\beta_a+1-y)$$\n",
    "\n",
    "where $y\\in\\{0,1\\}$ is the reward.\n",
    "Whenever we draw an arm and observe a reward (whenever we perform a Bernoulli trial), we update the posterior and use it as prior for the following draws from that arm.\n",
    "\n",
    "We can, however, take a different (but equivalent) approach where we keep the prior fixed and calculate the posterior using binomial likelihood for $n_a$ trials and $y_a$ successes for arm $a$.\n",
    "The posterior update step looks then as follows:\n",
    "\n",
    "$$p(\\theta_a|y_a) \\propto Beta(\\alpha+y_a,\\beta+n_a-y_a)$$\n",
    "\n",
    "for prior $Beta(\\alpha, \\beta)$.\n",
    "We use this latter approach in our implementation as it will later provide a convenient basis to extend the model with hierarchical prior.\n",
    "\n",
    "The main advantage of such a Bayesian framework is that it allows to quantify the uncertainty about $\\mathbf{\\theta}$ which would not be possible using frequentist approach.\n",
    "The posterior distribution of $\\mathbf{\\theta}$, not just a point estimate, gives a basis to guide the exploration using different approaches.\n",
    "In this project, we focus on two of them: Bayesian Upper Confidence Bound (Bayesian UCB) and Thompson sampling.\n",
    "\n",
    "\n",
    "## Bayesian UCB\n",
    "\n",
    "Bayesian UCB is, as the name implies, a Bayesian version of frequentist UCB and was proposed by Kaufmann et al[1].\n",
    "For each arm $a$, we obtain the value of a specific quantile, $q_a$: \n",
    "\n",
    "$q_a=Q(1-\\frac{1}{t(\\log n)^c}, \\theta_a)$\n",
    "\n",
    "where $(\\log n)^c$ is an artefact of the theoretical analysis, but the authors achieved most satisfying results with $c=0$ which simplifies the expression to:\n",
    "\n",
    "$q_a=Q(1-\\frac{1}{t},\\theta_a)$.\n",
    "\n",
    "Then, we draw the arm $a$ that has the highest value of $q_a$. After observing a new reward, we update the posterior distribution of $\\theta_a$.\n",
    "\n",
    "The fact that we use a high quantile, rather than the expectation ensures exploration, and is similar to the optimistic action selection that is used in frequentist UCB.\n",
    "\n",
    "## Thompson sampling\n",
    "\n",
    "Thompson sampling is a heurestic that uses the prior distribution to address the exploration-exploitation dilemma in a natural way. \n",
    "It choses the action that maximizes the expected reward with respect to a randomly drawn belief.\n",
    "In our K-MAB problem, we draw a sample $\\hat{\\mathbf{\\theta}}$ from the posterior distribution $\\mathbf{\\theta}$ and select the optimal action with respect to the model defined by $\\hat{\\mathbf{\\theta}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Bayesian K-MAB\n",
    "> TODO: Only use point estimates?\n",
    "\n",
    "In K-MAB problem, individual $\\theta_a$ probabilities are often drawn from a specific distribution, as is the case in our formulation.\n",
    "In the separate model presented above, we do not utilize this knowledge.\n",
    "However, if we use hierarchical approach, we could model the source distribution that generates $\\theta_a$ probabilities.\n",
    "In this project, for simplicity, we assume that we know that $\\theta_a$ probabilities are generated from Beta distribution($\\theta_a \\propto Beta(\\alpha,\\beta)$). To estimate $\\alpha$ and $\\beta$ parameters, we can use the distribution mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "$$\\mu = \\frac{\\alpha}{\\alpha + \\beta}$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha+\\beta+1)}$$\n",
    "\n",
    "Solving these equations for $\\alpha$ and $\\beta$ gives:\n",
    "\n",
    "$$\\beta = \\frac{\\mu(1-\\mu)^2}{\\sigma^2}-(1-\\mu)$$\n",
    "\n",
    "$$\\alpha = \\frac{\\mu \\beta}{1-\\mu}$$\n",
    "\n",
    "We calculate $\\mu$ and $\\sigma^2$ as follows:\n",
    "\n",
    "$$\\mu = E(\\theta)$$\n",
    "\n",
    "$$\\sigma^2 = Var(\\theta)$$\n",
    "\n",
    "where $\\mathbf{\\theta}$ is a vector with elements $\\frac{y_a}{n_a}$ for all $a \\in [1..K]$.\n",
    "\n",
    "We must note that this is not a fully Bayesian approach as $\\mathbf{\\theta}$ is a point estimate. A fully Bayesian approach would require numerical approximation to compute the posterior and as Stan does not yet allow to use posterior samples as prior for the new data (https://groups.google.com/forum/#!topic/stan-users/b0MWu4GJygI), we decided to build our prior only based on point estimate.\n",
    "\n",
    "We recalculate the hierarchical prior every time we pull an arm and observe a reward. The posterior is always calculated using the updated prior. Such a model is compatible with Bayesian UCB and Thompson sampling without any further modifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from agents import *\n",
    "except:\n",
    "    print('Download the code from github: https://github.com/Faur/Multi-Armed-Bayesian-Bandits')\n",
    "    print('or run the code block in the appendix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "> TODO: Can we get stan to work? http://andrewgelman.com/2018/02/04/andrew-vs-multi-armed-bandit/\n",
    "\n",
    "In the code below we run and compare the following agents:\n",
    " * Random agent \n",
    " * Frequentist UCB\n",
    " * Bayesian UCB \n",
    " * Hierarchical Bayesian UCB \n",
    " * Thompson Sampling\n",
    " * Hierarchical Thompson Sampling\n",
    "\n",
    "for $k = \\{2, 4, 8, 16, 32\\}$.\n",
    "The for each $k$ the experiment is repeated 500 times, and the mean regret is plotted.\n",
    "(Standard deviations have been left out of the plots, as they make the plots cluttered.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def make_agents(env, k, max_steps):\n",
    "    agents = [OptimalAgent(env), RandomAgent(k), FreqUCB(k), BayesUCB(k, max_steps), HierarchicalBayesUCB(k, max_steps), ThompsonSampling(k, max_steps), HierarchicalThompsonSampling(k, max_steps)]\n",
    "    return agents\n",
    "\n",
    "def run(args):\n",
    "    \"\"\" Run all the agnets in 'make_agents' once.\"\"\"\n",
    "    k, max_steps, alpha, beta = args \n",
    "    env = KBandit(k, max_steps=max_steps, alpha=alpha, beta=beta)\n",
    "    agents = make_agents(env, k, max_steps)\n",
    "\n",
    "    rewards = []\n",
    "    _, _, d, _ = env.reset()\n",
    "    for a in agents:\n",
    "        a.reset()\n",
    "    while not d:\n",
    "        draw = True\n",
    "        rewards_ = []\n",
    "        for agent in agents:\n",
    "            a = agent.action()\n",
    "            _, r, d, _ = env.step(a, draw)\n",
    "            draw = False\n",
    "            agent.update(a, r)\n",
    "            rewards_.append(r)\n",
    "        rewards.append(rewards_)\n",
    "\n",
    "    return np.array(rewards).T\n",
    "\n",
    "def runs(num_runs, args, mp=False):\n",
    "    \"\"\" Run num_runs experiments, either sequentially or using multiprocessing.\"\"\"\n",
    "    try:\n",
    "        if mp:\n",
    "            print('multiprocessing')\n",
    "            proc_pool = Pool()\n",
    "            rewards = proc_pool.map(run, [args for i in range(num_runs)])\n",
    "        else:\n",
    "            print('single thread')\n",
    "            rewards = []\n",
    "            for i in range(num_runs):\n",
    "                print('\\rrun', i+1, 'of', num_runs, end=''); sys.stdout.flush()\n",
    "                rewards.append(run(args))\n",
    "            print()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "    # rewards = np.mean(rewards, 0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def plot_cumsum(rewards, agents_names, use_std=False):\n",
    "    \"\"\" Plotting helper function.\"\"\"\n",
    "    colors=plt.cm.rainbow(np.linspace(0,1,rewards.shape[1]))\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for i in range(len(agents_names)-1):\n",
    "    #     plt.plot(np.cumsum(rewards[0] - rewards[i+1]), label=agents_names[i+1]+' regret')\n",
    "        c = colors[i]\n",
    "        cumsums = np.cumsum(rewards[:,0,:] - rewards[:,i+1,:], -1)\n",
    "        means = np.mean(cumsums, 0)\n",
    "        stds = np.std(cumsums, 0)\n",
    "        plt.plot(means, c=c, lw=2, label=agents_names[i+1], alpha=0.75)\n",
    "        if use_std:\n",
    "            plt.plot(means+stds, c=c, linestyle='--', alpha=0.5)\n",
    "            plt.plot(means-stds, c=c, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NB:** [Multiprocessing is strange on **Windows**](https://docs.python.org/2/library/multiprocessing.html#windows), and can therefore not be run inside a Jupyter Notebook. Use `useMultiProcess = False`, or run the `run.py` file from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## run.py\n",
    "\n",
    "show_visualization = True\n",
    "save_rewards = False\n",
    "useMultiProcess = True\n",
    "\n",
    "max_steps = 500\n",
    "num_episodes = 500\n",
    "\n",
    "ks = [2, 4, 8, 16, 32]\n",
    "\n",
    "print('save_rewards', save_rewards)\n",
    "print('max_steps', max_steps)\n",
    "print('num_episodes', num_episodes)\n",
    "print('ks', ks)\n",
    "print()\n",
    "\n",
    "agents_names = ['Optimal', 'Random', 'FreqUCB', 'BayesUCB', 'HierarchicalBayesUCB', 'ThompsonSampling', 'HierarchicalThompsonSampling']\n",
    "for k in ks:\n",
    "    print('k =',k)\n",
    "    alpha = 10; beta = 40\n",
    "    args = (k, max_steps, alpha, beta)\n",
    "\n",
    "    rewards = runs(num_episodes, args, mp=useMultiProcess)\n",
    "    if save_rewards:\n",
    "        save_name = 'raw_rewards_'+str(k)+'.npy'\n",
    "        print('saving', save_name)\n",
    "        np.save(save_name, rewards)\n",
    "\n",
    "    plot_cumsum(rewards, agents_names)\n",
    "    plt.title('Regret, k='+str(k))\n",
    "    plt.draw()\n",
    "\n",
    "    plot_cumsum(rewards, agents_names, True)\n",
    "    plt.title('Regret, k='+str(k))\n",
    "    plt.draw()\n",
    "    print()\n",
    "\n",
    "if show_visualization:\n",
    "    plt.show()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The experiments take quite a long time to finish, and were therefore run on the DTU servers.\n",
    "The results (as reported by `plot_cum_sum`) are reproduced here.\n",
    "\n",
    "![](./img/collection.png)\n",
    "\n",
    "Looking at these pictures, we can observe several things about the performance.\n",
    "1. As $k$ increases the problem becomes more difficult, as indicated by the increasing rate of regret.\n",
    "This is becaues there are more arms to test, and the difference between a random arm, and the best arm becomes larger.\n",
    "This makes convergence to take longer, and we can clearly see that for the higer values of $k$ the agents haven't fully converged after 500 steps.\n",
    "(Due to time constraints we were unable to run the model for longer.)\n",
    "\n",
    "* Frequentist UCB is consistently better than random and worse than the Bayesian approaches.\n",
    "* As $k$ increases the benifit of the hierarchical approach becomes more and more apparent, as it is able to learn quicker, by generalizing between arms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMMENTS AND NOTES\n",
    "\n",
    "Posterior predictive checking\n",
    "* Does this make sense in BRL? We are not really interested in accurately monitoring the world, rather we are interested in finding the best action\n",
    "* Perhaps determining the accuracy on the most frequent action is a good idea?\n",
    "* in hierarchical, does the shared prior end up matching a=10, b=40?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and discussion\n",
    " * The main conclusion of the data analysis should be clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Kaufmann, E., Garivier, A., & Paristech, T. (2013). On bayesian upper confidence bounds for bandit problems. https://doi.org/10.1.1.392.8327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### agents.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### bandit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
