{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Authors: Toke~ Faurby~ (s136232)~ and~ Maciej~ Korzepa~ (mjko).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important notes\n",
    " * illustrate the knowledge of the Bayesian workflow.\n",
    " * Structure: The notebook presents a clear cohesive data analysis story, which is enjoyable to read\n",
    " * Statistical terms: Statistical terms are used accurately and with clarity\n",
    "\n",
    "#### TODO:\n",
    " * Description of Bayesian UCB\n",
    " * Description of Thompson Sampling\n",
    " * Description of Hierarchical extensions\n",
    " * Description of the prior\n",
    " * Inlude code\n",
    " * Posterior predictive checking\n",
    " * Model comparison if applicable (e.g. with loo)\n",
    " * Predictive performance assessment if applicable (e.g. classification accuracy)\n",
    " * Potentially sensitivity analysis\n",
    " * Discussion of problems, and potential improvements\n",
    " * Convergence diagnostics (Rhat, divergences, neff)\n",
    " * Conclusion:\n",
    "     * The main conclusion of the data analysis should be clear\n",
    "\n",
    "\n",
    "Tokes job\n",
    " * Run excisting code\n",
    " * Give Stan a chance\n",
    "\n",
    "\n",
    "#### DONE: \n",
    " * Introduction: \n",
    "     * The introduction is inviting, presents an overview of the notebook. Information is relevant and presented in a logical order.\n",
    " * Description of the data\n",
    " * Description of analysis problem\n",
    " * Description of Frequentis UCB\n",
    "\n",
    "\n",
    "Things we skip\n",
    " * Stan Code \n",
    " * How Stan model is run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Reinforcement learning** is about goal directed learning from interactions, and is generally thought of as distinct from supervised and unsupervised learning.\n",
    "The goal is to learn what actions to take, and when to take them, so as to optimize long-term performance.\n",
    "This may involve sacrificing immediate reward to obtain greater reward in the long-term or just to obtain more information about the environment.\n",
    "The tradeoff between maximizing reward and learning about the environment is called the _exploration/exploitation dillemma_, and it is one of the problems at the core of reinforcement learning.\n",
    "\n",
    "\n",
    "**Bayesian methods** uses Bayes rule to define a posterior distribution, based on a prior on the model parameters and the likelihood of the obsevations given model parameters.\n",
    "Should we obtain even more observations, the old posterior becomes the new prior and the process is repeated.\n",
    "This property makes Bayesian models attractive for reinforcement learning, as the ability to effectively utilize new observations as they become available is important for overall performance (unlike many other data analysis problems).\n",
    "This explicit modelling of probabilities gives rise to principled methods for incorporating prior information and action-selection (exploration/exploitation) as a function of the uncertainty in learning.\n",
    "Bayesian methods also enable hierarchical approaches, that enhace data efficiency even further.\n",
    "\n",
    "\n",
    "In this notebook we will show the principles of **Bayesain reinforcement learning** using the classic and very simple problem - the **multi-armed bandit problem**.\n",
    "At each time step the agent must select one of $K$ arms, and will subsequently recieve a reward based on some unknown probability distribution $p(\\theta_k)$.\n",
    "The goal is to get as high a total reward as possible.\n",
    "The performance is often measured in terms of expected regret, i.e. the difference between the selected actions, and the optimal action.\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left(Regret(T)\\right) \n",
    "=\n",
    "\\mathbb{E} \\left[ \\sum_{t=1}^T \\left( r(a^*) - r(a_t) \\right)\\right]\n",
    "$$\n",
    "\n",
    "where $r(a)$ is the recieved reward after performing action $a$. \n",
    "$a^*$ is the action that gives the highest expected reward.\n",
    "\n",
    "The environment is therefore static, greatly simplifying the problem, but many of the general properties still hold.\n",
    "\n",
    "\n",
    "## Content\n",
    "1. The Environment: Multi-Armed Bernoulli Bandits\n",
    "* The Agents - Frequentist baseline, Bayesian approach, and hierarchical Bayesian approach\n",
    "* Implementation - what we did and how it works\n",
    "* Results\n",
    "* Conclusion and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Environment: Multi-Armed Bernoulli Bandits\n",
    "\n",
    "The **data** is obtained iteratively through interacting with the environment, which we define as follows:\n",
    "\n",
    "When initialized $K$ parameters $\\theta_k$ are sampled from a $Beta(\\alpha, \\beta)$ distribution, with $\\alpha=10$ and $\\beta=40$ (these values are arbitrary, but we don't want to use a uniform distribution, as it would then match our prior).\n",
    "One episode lasts $5000$ steps, and at each time step, $t$ the agent picks an action, $a_t$, and recieves a reward, sampled from $Bernoulli(\\theta_{a_t})$.\n",
    "\n",
    "The agent must therefore balance selecting arms that it believes are good (exploit high $\\theta_k$) and unknown arms that might be even better (explore uncertain $\\theta_k$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agents\n",
    " * Did you get a sense of what is the model? Where and how might the author make the model description more clear?\n",
    "\n",
    "In order to demonstrate the effectiveness of Bayesian approaches we use the popular frequentist *upper confidence bound* (UCB) method as a benchmark.\n",
    "For Bayesian approaches we have selected '*Bayesian UCB*', for easy direct comparison, and '*Thompson Sampling*' (aka. probability matching), which achieves state of the art performance on this task.\n",
    "We also examin the hierarchical extensions of the two Bayesian approaches.\n",
    "\n",
    "\n",
    "## Frequentist UCB\n",
    "The idea behind frequentist UCB is that we shuold always explore some, as we can never be certain whether or not we have foun the optimal arms.\n",
    "Exploratory actions should be selected based on their potential for being optimal, taking into account both how close their estimates are to being optimal and the uncertainties in those estimates.\n",
    "This is done using the following equation:\n",
    "\n",
    "$$\n",
    "a_t = \\arg \\max_a \\left[ Q(a) + c\\sqrt{\\frac{\\ln{t}}{N(a)}} \\right]\n",
    "$$\n",
    "\n",
    "Where $Q(a)$ is the emperical mean reward of action $a$, $c$ is a hyperparameter determining the confidence level, and $N(a)$ returns the number of times action $a$ has been selected.\n",
    "\n",
    "Each time $a$ is selected the associated is reduced ($N(a)$ increases).\n",
    "Similarly when an action other than $a$ is selected  the uncertainty estimate increases ($t$ increases)\n",
    "The natural logarithm insures that the means that this increase in uncertainty gets smaller over time, but it is still unbounded, meaning that in the limit all actions will be taken infinitely many times (constant exploration).\n",
    "\n",
    "\n",
    "## Bayesian UCB\n",
    " * TODO: Priors are listed and justified\n",
    "\n",
    "## Thompson Sampling\n",
    "\n",
    "\n",
    "## Hierarchical extensions\n",
    "* TODO: Only use point estimates?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "> NB: Two files `agents.py` and `bandit.py` hold the code describing the agents and bandit environment respectively. The code is reproduced at the end of the notebook (in case you want to run it, run those first). The code is also available [here](https://github.com/Faur/Multi-Armed-Bayesian-Bandits). \n",
    "\n",
    "* TODO: Can we get stan to work? http://andrewgelman.com/2018/02/04/andrew-vs-multi-armed-bandit/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and discussion\n",
    " * The main conclusion of the data analysis should be clear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### agents.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### bandit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
