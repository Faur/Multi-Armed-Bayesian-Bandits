{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Authors: Toke~ Faurby~ (s136232)~ and~ Maciej~ Korzepa~ (mjko).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "**Reinforcement learning** is about goal directed learning from interactions, and is generally thought of as distinct from supervised and unsupervised learning.\n",
    "The goal is to learn what actions to take, and when to take them, so as to optimize long-term performance.\n",
    "This may involve sacrificing immediate reward to obtain greater reward in the long-term or just to obtain more information about the environment.\n",
    "The tradeoff between maximizing reward and learning about the environment is called the _exploration/exploitation dillemma_, and it is one of the problems at the core of reinforcement learning.\n",
    "\n",
    "\n",
    "**Bayesian methods** uses Bayes rule to define a posterior distribution, based on a prior on the model parameters and the likelihood of the obsevations given model parameters.\n",
    "Should we obtain even more observations, the old posterior becomes the new prior and the process is repeated.\n",
    "This property makes Bayesian models attractive for reinforcement learning, as the ability to effectively utilize new observations as they become available is important for overall performance (unlike many other data analysis problems).\n",
    "This explicit modelling of probabilities gives rise to principled methods for incorporating prior information and action-selection (exploration/exploitation) as a function of the uncertainty in learning.\n",
    "Bayesian methods also enable hierarchical approaches, that enhace data efficiency even further.\n",
    "\n",
    "\n",
    "In this notebook we will show the principles of **Bayesain reinforcement learning** using the classic and very simple problem - the **multi-armed bandit problem**.\n",
    "At each time step the agent must select one of $K$ arms, and will subsequently recieve a reward based on some unknown probability distribution $p(\\theta_k)$.\n",
    "The goal is to get as high a total reward as possible.\n",
    "The performance is often measured in terms of expected regret, i.e. the difference between the selected actions, and the optimal action.\n",
    "\n",
    "$$\n",
    "\\mathbb{E} \\left(Regret(T)\\right) \n",
    "=\n",
    "\\mathbb{E} \\left[ \\sum_{t=1}^T \\left( r(a^*) - r(a_t) \\right)\\right]\n",
    "$$\n",
    "\n",
    "where $r(a)$ is the recieved reward after performing action $a$. \n",
    "$a^*$ is the action that gives the highest expected reward.\n",
    "\n",
    "The environment is therefore static, greatly simplifying the problem, but many of the general properties still hold.\n",
    "\n",
    "\n",
    "## Content\n",
    "1. The Environment: Multi-Armed Bernoulli Bandits\n",
    "* The Agents - Frequentist baseline, Bayesian approach, and hierarchical Bayesian approach\n",
    "* Implementation - what we did and how it works\n",
    "* Results\n",
    "* Conclusion and discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Environment: Multi-Armed Bandits (K-MAB)\n",
    "\n",
    "The **data** is obtained iteratively through interacting with the environment, which we define as follows:\n",
    "\n",
    "When initialized $K$ parameters $\\theta_k$ are sampled from a $Beta(\\alpha, \\beta)$ distribution, with $\\alpha=10$ and $\\beta=40$ (these values are arbitrary, but we don't want to use a uniform distribution, as it would then match our prior).\n",
    "One episode lasts $1000$ steps, and at each time step, $t$ the agent picks an action, $a_t$, and recieves a reward, sampled from a Bernoulli distribution, with parameter $\\theta_{a_t}$.\n",
    "\n",
    "The agent must therefore balance selecting arms that it believes are good (exploit high expected $\\theta_k$) and unknown arms that might be even better (explore uncertain $\\theta_k$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NB:** Two files `agents.py` and `bandit.py` hold the code describing the agents and bandit environment respectively. The code is reproduced at the end of the notebook (in case you want to run it, run those first). The code is also available [here](https://github.com/Faur/Multi-Armed-Bayesian-Bandits). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from bandit import *\n",
    "except:\n",
    "    print('Download the code from github: https://github.com/Faur/Multi-Armed-Bayesian-Bandits')\n",
    "    print('or run the code block in the appendix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Agents\n",
    "In order to demonstrate the effectiveness of Bayesian approaches we use the popular frequentist *upper confidence bound* (UCB) method as a benchmark.\n",
    "For Bayesian approaches we have selected '*Bayesian UCB*', for easy direct comparison, and '*Thompson Sampling*' (aka. probability matching), which achieves state of the art performance on the $k=2$ task.\n",
    "We also examin the hierarchical extensions of the two Bayesian approaches.\n",
    "\n",
    "\n",
    "## Frequentist UCB\n",
    "The idea behind frequentist UCB is that we should always explore to some extent, as we can never be certain whether or not we have foun the optimal arms.\n",
    "Exploratory actions should be selected based on their potential for being optimal, taking into account both how close their estimates are to being optimal and the uncertainties in those estimates.\n",
    "This is done through optimistic action selection, using the following selection criteria:\n",
    "\n",
    "$$\n",
    "a_t = \\arg \\max_a \\left[ \\overline{r(a)} + c\\sqrt{\\frac{\\ln{t}}{N(a)}} \\right]\n",
    "$$\n",
    "\n",
    "Where $\\overline{r(a)}$ is the emperical mean reward recieved from performing action $a$, $c$ is a hyperparameter determining the confidence level, and $N(a)$ returns the number of times action $a$ has been selected.\n",
    "\n",
    "Each time $a$ is selected the associated is reduced ($N(a)$ increases).\n",
    "Similarly when an action other than $a$ is selected  the uncertainty estimate increases ($t$ increases)\n",
    "The natural logarithm insures that the means that this increase in uncertainty gets smaller over time, but it is still unbounded, meaning that in the limit all actions will be taken infinitely many times (constant exploration).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian Approach\n",
    "According to K-MAB model, the only unknown quantities are action dependent outcome probabilities $P(\\cdot | a)$. \n",
    "We can learn these probabilities using Bayesian interference during sequential interaction with the MAB.\n",
    "For our Bernoulli K-MAB model, action outcome probabilities are parametrized by vector $\\mathbf{\\theta} \\in [0,1]^{K}$.\n",
    "As each interaction with the model corresponds to performing a Bernoulli trial, the natural choice of prior for $\\mathbf{\\theta}$ is Beta distribution for each arm $a$ i.e. \n",
    "\n",
    "$$p(\\theta_a) \\sim Beta(\\alpha_a,\\beta_a)$$\n",
    "\n",
    "The prior for $\\theta_a$ is chosen to be a non-informative prior $Beta(1,1)$, as we don't have any prior knowledge about possible values of $\\theta_a$.\n",
    "As this is also conjugate prior for bernoulli distribution, we can easily calculate the posterior of $\\theta_a$ given one observation:\n",
    "\n",
    "$$p(\\theta_a|y) \\propto Beta(\\alpha_a+y,\\beta_a+1-y)$$\n",
    "\n",
    "where $y\\in\\{0,1\\}$ is the reward.\n",
    "Whenever we draw an arm and observe a reward (whenever we perform a Bernoulli trial), we update the posterior and use it as prior for the following draws from that arm.\n",
    "\n",
    "We can, however, take a different (but equivalent) approach where we keep the prior fixed and calculate the posterior using binomial likelihood for $n_a$ trials and $y_a$ successes for arm $a$.\n",
    "The posterior update step looks then as follows:\n",
    "\n",
    "$$p(\\theta_a|y_a) \\propto Beta(\\alpha+y_a,\\beta+n_a-y_a)$$\n",
    "\n",
    "for prior $Beta(\\alpha, \\beta)$.\n",
    "We use this latter approach in our implementation as it will later provide a convenient basis to extend the model with hierarchical prior.\n",
    "\n",
    "The main advantage of such a Bayesian framework is that it allows to quantify the uncertainty about $\\mathbf{\\theta}$ which would not be possible using frequentist approach.\n",
    "The posterior distribution of $\\mathbf{\\theta}$, not just a point estimate, gives a basis to guide the exploration using different approaches.\n",
    "In this project, we focus on two of them: Bayesian Upper Confidence Bound (Bayesian UCB) and Thompson sampling.\n",
    "\n",
    "\n",
    "## Bayesian UCB\n",
    "\n",
    "Bayesian UCB is, as the name implies, a Bayesian version of frequentist UCB and was proposed by Kaufmann et al[1].\n",
    "For each arm $a$, we obtain the value of a specific quantile, $q_a$: \n",
    "\n",
    "$q_a=Q(1-\\frac{1}{t(\\log n)^c}, \\theta_a)$\n",
    "\n",
    "where $(\\log n)^c$ is an artefact of the theoretical analysis, but the authors achieved most satisfying results with $c=0$ which simplifies the expression to:\n",
    "\n",
    "$q_a=Q(1-\\frac{1}{t},\\theta_a)$.\n",
    "\n",
    "Then, we draw the arm $a$ that has the highest value of $q_a$. After observing a new reward, we update the posterior distribution of $\\theta_a$.\n",
    "\n",
    "The fact that we use a high quantile, rather than the expectation ensures exploration, and is similar to the optimistic action selection that is used in frequentist UCB.\n",
    "\n",
    "## Thompson sampling\n",
    "\n",
    "Thompson sampling is a heurestic that uses the prior distribution to address the exploration-exploitation dilemma in a natural way. \n",
    "It choses the action that maximizes the expected reward with respect to a randomly drawn belief.\n",
    "In our K-MAB problem, we draw a sample $\\hat{\\mathbf{\\theta}}$ from the posterior distribution $\\mathbf{\\theta}$ and select the optimal action with respect to the model defined by $\\hat{\\mathbf{\\theta}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Bayesian K-MAB\n",
    "\n",
    "In K-MAB problem, individual $\\theta_a$ probabilities are often drawn from a specific distribution, as is the case in our formulation.\n",
    "In the separate model presented above, we do not utilize this knowledge.\n",
    "However, if we use hierarchical approach, we could model the source distribution that generates $\\theta_a$ probabilities.\n",
    "In this project, for simplicity, we assume that we know that $\\theta_a$ probabilities are generated from Beta distribution($\\theta_a \\propto Beta(\\alpha,\\beta)$). To estimate $\\alpha$ and $\\beta$ parameters, we can use the distribution mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "$$\\mu = \\frac{\\alpha}{\\alpha + \\beta}$$\n",
    "\n",
    "$$\\sigma^2 = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha+\\beta+1)}$$\n",
    "\n",
    "Solving these equations for $\\alpha$ and $\\beta$ gives:\n",
    "\n",
    "$$\\beta = \\frac{\\mu(1-\\mu)^2}{\\sigma^2}-(1-\\mu)$$\n",
    "\n",
    "$$\\alpha = \\frac{\\mu \\beta}{1-\\mu}$$\n",
    "\n",
    "We calculate $\\mu$ and $\\sigma^2$ as follows:\n",
    "\n",
    "$$\\mu = E(\\theta)$$\n",
    "\n",
    "$$\\sigma^2 = Var(\\theta)$$\n",
    "\n",
    "where $\\mathbf{\\theta}$ is a vector with elements $\\frac{y_a}{n_a}$ for all $a \\in [1..K]$.\n",
    "\n",
    "We must note that this is not a fully Bayesian approach as $\\mathbf{\\theta}$ is a point estimate. A fully Bayesian approach would require numerical approximation to compute the posterior and as Stan does not yet allow to use posterior samples as prior for the new data (https://groups.google.com/forum/#!topic/stan-users/b0MWu4GJygI), we decided to build our prior only based on point estimate.\n",
    "\n",
    "We recalculate the hierarchical prior every time we pull an arm and observe a reward. The posterior is always calculated using the updated prior. Such a model is compatible with Bayesian UCB and Thompson sampling without any further modifications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from agents import *\n",
    "except:\n",
    "    print('Download the code from github:', \n",
    "          'https://github.com/Faur/Multi-Armed-Bayesian-Bandits')\n",
    "    print('or run the code block in the appendix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "In the code below we run and compare the following agents:\n",
    " * Random agent \n",
    " * Frequentist UCB\n",
    " * Bayesian UCB \n",
    " * Hierarchical Bayesian UCB \n",
    " * Thompson Sampling\n",
    " * Hierarchical Thompson Sampling\n",
    "\n",
    "for $k = \\{2, 4, 8, 16, 32\\}$.\n",
    "The for each $k$ the experiment is repeated 500 times, and the mean regret is plotted.\n",
    "(Standard deviations have been left out of the plots, as they make the plots cluttered.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Helper functions\n",
    "\n",
    "def make_agents(env, k, max_steps):\n",
    "    agents = [OptimalAgent(env), \n",
    "              RandomAgent(k), \n",
    "              FreqUCB(k), \n",
    "              BayesUCB(k, max_steps), \n",
    "              HierarchicalBayesUCB(k, max_steps), \n",
    "              ThompsonSampling(k, max_steps), \n",
    "              HierarchicalThompsonSampling(k, max_steps)]\n",
    "    \n",
    "    return agents\n",
    "\n",
    "def run(args):\n",
    "    \"\"\" Run all the agnets in 'make_agents' once.\"\"\"\n",
    "    k, max_steps, alpha, beta = args \n",
    "    env = KBandit(k, max_steps=max_steps, alpha=alpha, beta=beta)\n",
    "    agents = make_agents(env, k, max_steps)\n",
    "\n",
    "    rewards = []\n",
    "    _, _, d, _ = env.reset()\n",
    "    for a in agents:\n",
    "        a.reset()\n",
    "    while not d:\n",
    "        draw = True\n",
    "        rewards_ = []\n",
    "        for agent in agents:\n",
    "            a = agent.action()\n",
    "            _, r, d, _ = env.step(a, draw)\n",
    "            draw = False\n",
    "            agent.update(a, r)\n",
    "            rewards_.append(r)\n",
    "        rewards.append(rewards_)\n",
    "\n",
    "    return np.array(rewards).T\n",
    "\n",
    "def runs(num_runs, args, mp=False):\n",
    "    \"\"\" Run num_runs experiments, either sequentially \n",
    "    or using multiprocessing.\"\"\"\n",
    "    try:\n",
    "        if mp:\n",
    "            print('multiprocessing')\n",
    "            proc_pool = Pool()\n",
    "            rewards = proc_pool.map(run, [args for i in range(num_runs)])\n",
    "        else:\n",
    "            print('single thread')\n",
    "            rewards = []\n",
    "            for i in range(num_runs):\n",
    "                print('\\rrun', i+1, 'of', num_runs, end=''); sys.stdout.flush()\n",
    "                rewards.append(run(args))\n",
    "            print()\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    rewards = np.array(rewards)\n",
    "    # rewards = np.mean(rewards, 0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def plot_cumsum(rewards, agents_names, use_std=False):\n",
    "    \"\"\" Plotting helper function.\"\"\"\n",
    "    colors=plt.cm.rainbow(np.linspace(0,1,rewards.shape[1]))\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for i in range(len(agents_names)-1):\n",
    "        c = colors[i]\n",
    "        cumsums = np.cumsum(rewards[:,0,:] - rewards[:,i+1,:], -1)\n",
    "        means = np.mean(cumsums, 0)\n",
    "        stds = np.std(cumsums, 0)\n",
    "        plt.plot(means, c=c, lw=2, label=agents_names[i+1], alpha=0.75)\n",
    "        if use_std:\n",
    "            plt.plot(means+stds, c=c, linestyle='--', alpha=0.5)\n",
    "            plt.plot(means-stds, c=c, linestyle='--', alpha=0.5)\n",
    "\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NB:** [Multiprocessing is strange on **Windows**](https://docs.python.org/2/library/multiprocessing.html#windows), and can therefore not be run inside a Jupyter Notebook. Use `useMultiProcess = False`, or run the `run.py` file from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run.py\n",
    "\n",
    "show_visualization = True\n",
    "save_rewards = False\n",
    "useMultiProcess = True\n",
    "\n",
    "max_steps = 500\n",
    "num_episodes = 500\n",
    "\n",
    "ks = [2, 4, 8, 16, 32]\n",
    "\n",
    "print('save_rewards', save_rewards)\n",
    "print('max_steps', max_steps)\n",
    "print('num_episodes', num_episodes)\n",
    "print('ks', ks)\n",
    "print()\n",
    "\n",
    "agents_names = ['Optimal', \n",
    "                'Random', \n",
    "                'FreqUCB', \n",
    "                'BayesUCB', \n",
    "                'HierarchicalBayesUCB', \n",
    "                'ThompsonSampling', \n",
    "                'HierarchicalThompsonSampling']\n",
    "for k in ks:\n",
    "    print('k =',k)\n",
    "    alpha = 10; beta = 40\n",
    "    args = (k, max_steps, alpha, beta)\n",
    "\n",
    "    rewards = runs(num_episodes, args, mp=useMultiProcess)\n",
    "    if save_rewards:\n",
    "        save_name = 'raw_rewards_'+str(k)+'.npy'\n",
    "        print('saving', save_name)\n",
    "        np.save(save_name, rewards)\n",
    "\n",
    "    plot_cumsum(rewards, agents_names)\n",
    "    plt.title('Regret, k='+str(k))\n",
    "    plt.draw()\n",
    "\n",
    "    plot_cumsum(rewards, agents_names, True)\n",
    "    plt.title('Regret, k='+str(k))\n",
    "    plt.draw()\n",
    "    print()\n",
    "\n",
    "if show_visualization:\n",
    "    plt.show()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "The experiments take quite a long time to finish, and were therefore run on the DTU servers.\n",
    "The results (as reported by `plot_cum_sum`) are reproduced here.\n",
    "\n",
    "Unfortunately, just before the submission, we realised that jupyter doesn't generate a pdf correctly with our locally linked images. The picture collecting all plots is stored in a dropbox folder instead. If possible, please open the link and see the plots separately. We're sorry for the inconvenience :( \n",
    "\n",
    "https://www.dropbox.com/sh/1fxnmlqmripleu4/AABoXmzPQHtKPkSzwobpNEeKa?dl=0\n",
    "\n",
    "\n",
    "Looking at these pictures, we can observe several things about the performance.\n",
    "1. As $k$ increases the problem becomes more difficult, as indicated by the increasing rate of regret.\n",
    "This is becaues there are more arms to test, and the difference between a random arm, and the best arm becomes larger.\n",
    "This makes convergence to take longer, and we can clearly see that for the higer values of $k$ the agents haven't fully converged after 500 steps.\n",
    "(Due to time constraints we were unable to run the model for longer.)\n",
    "\n",
    "* Frequentist UCB is consistently better than random and worse than the Bayesian approaches.\n",
    "* As $k$ increases the benifit of the hierarchical approach becomes more and more apparent, as they are able to learn quicker, by generalizing between arms. Further more for $k=16,~k=32$ hierarchical Bayesian UCB seems to perform better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion and conclusion\n",
    "Multi-armed bandits are however very simple and naturally lend themselves very well to Bayesian modelling.\n",
    "As we have shown this simple problem nicely illustrates the how a Bayesian approach enables new methods that are able to use the data more efficiently, and thereby improve performance.\n",
    "This is especially apparent for the hierarchical methods when $k$ is large.\n",
    "Bayesian approach outperformed the frequentist one in all simulations by a considerable margin. We believe that the main factor that allowed this was the ability of Bayesian simulation to explicitely quantify the uncertainty in this stochastic environment. \n",
    "\n",
    "We should also discuss the choice of $\\mathbf{\\theta}$ probabilities for bandit's arms. In this project, we do it stochastically in each episode based on beta distribution with fixed parameters. In Kaufmann's paper[1], the authors used fixed probabilities ($\\theta_1=0.1$, $\\theta_2=0.2$ - only two arms were tested). We believe that using fixed probabilities would decrease the number of episodes needed to achieve stable results. Right now, an episode might have let's say 0.1 and 0.3 while another 0.19, 0.20 - these runs will produce completely different results so we might need lots of episodes to average over the beta distribution we use. \n",
    "\n",
    "It is also worth noting that if we have more arms and we draw thetas from a specific distribution, we will have a larger set of thetas that are high and similar to each other. In this case, it takes long to find the optimal arm, but we should also note that as the arms that are tried most often also have very high rates so the difference between the optimal strategy and our strategy will be very small.\n",
    "\n",
    "If we had more time for this project, we would try to find a way (or develop our own sampler as we did in one of the assignments) to evaluate the hierarchical prior not based on point estimate but rather on full probability distributions for $\\theta$ parameters for each arm and compare it to the other models. It would be also interesting to show how expected posterior $\\theta$ change after each draw for different methods that we used.\n",
    "\n",
    "It is possible to use Bayesian approaches for more complicated problems, but it quickly becomes very difficuly to setup density functions for all the relevant quantities.\n",
    "Another issue with Bayesian methods is computational efficiency.\n",
    "Even though they are generally quite data efficient it does come at a computational cost.\n",
    "Reinforcement learning generally requires a very high number of observations, which could lead to problems unless we manage to construct models for which posterior has a closed-form solution (as this problem).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Kaufmann, E., Garivier, A., & Paristech, T. (2013). On bayesian upper confidence bounds for bandit problems. https://doi.org/10.1.1.392.8327"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## bandit.py \n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "class KBandit():\n",
    "    \"\"\" Bernuli multiarmed bandit with Beta prior. \n",
    "        Example 3.1 of \"Bayesian Reinforcement Learning.\"\n",
    "        \n",
    "        Implementation is slightly different than the Model 5 \n",
    "        (Simple Family of Alternative Bandit Processes)\n",
    "        (page 32)\n",
    "    \"\"\"\n",
    "    def __init__(self, k, alpha=1, beta=1, max_steps=None):\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        self.dist = beta_dist(self.alpha, self.beta)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self._theta = self.dist.rvs(self.k)\n",
    "        return None, 0, False, None\n",
    "\n",
    "    def _draw(self):\n",
    "        draws = np.random.uniform(size=self.k)\n",
    "        # print(draws)\n",
    "        self._reward_table = (draws < self._theta).astype(int)\n",
    "\n",
    "    def optimal_action(self):\n",
    "        return np.argmax(self._theta)\n",
    "    \n",
    "    def step(self, action, draw=True):\n",
    "        \"\"\" If draw=False the step isn't counted, and the draws from last step are reused.\n",
    "            This makes it easier to fairly compare algorithms.\n",
    "        \"\"\"\n",
    "        if draw:\n",
    "            self._draw()\n",
    "            self.cur_step += 1\n",
    "        \n",
    "        s, i = None, None\n",
    "        r = self._reward_table[action]\n",
    "        d = True if self.max_steps and self.cur_step > self.max_steps else False\n",
    "        return s, r, d, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## agents.py \n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "class RandomAgent():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "    def reset(self):\n",
    "        pass\n",
    "    def action(self):\n",
    "        return np.random.randint(self.k)\n",
    "    def update(self, *args):\n",
    "        pass\n",
    "\n",
    "class OptimalAgent():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def reset(self):\n",
    "        pass\n",
    "    def action(self):\n",
    "        return self.env.optimal_action()\n",
    "    def update(self, *args):\n",
    "        pass\n",
    "\n",
    "class FreqUCB():\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.means = np.zeros(self.k)\n",
    "        self.pulls = np.zeros(self.k)\n",
    "    \n",
    "    def action(self):\n",
    "        if self.steps < self.k:\n",
    "            # Pull each arm once initially\n",
    "            return self.steps\n",
    "        \n",
    "        a = np.argmax(self.means + np.sqrt(2*np.log(self.steps)/self.pulls))\n",
    "        return a\n",
    "            \n",
    "    def update(self, action, reward):\n",
    "        self.steps += 1\n",
    "        self.pulls[action] += 1\n",
    "        self.means[action] = self.means[action]*(self.pulls[action]-1) /\n",
    "            (self.pulls[action]) + reward/(self.pulls[action])\n",
    "\n",
    "class BayesUCB():\n",
    "    def __init__(self, k, n, c=0):\n",
    "        self.k = k\n",
    "        self.n = n # UCB param - Episode length\n",
    "        self.c = c # UCB param ~ Exploration parameter\n",
    "\n",
    "    @property\n",
    "    def prior_param(self):\n",
    "        return [1,1]\n",
    "\n",
    "    def reset(self):\n",
    "        self.t=0\n",
    "        self.post_param = np.zeros([self.k])\n",
    "        self.draws = np.zeros([self.k])\n",
    "\n",
    "    def compute_ab(self, i):\n",
    "        a = self.prior_param[0]+self.post_param[i]\n",
    "        b = self.prior_param[1]+self.draws[i]-self.post_param[i]\n",
    "        assert a > 0, 'a = {}, prior {}, post {}'.format(a, self.prior_param[0], \n",
    "                                                         self.post_param[i])\n",
    "        assert b > 0, 'b = {}, prior {}, post {}'.format(b, self.prior_param[1], \n",
    "                                                         self.draws[i]-self.post_param[i])\n",
    "        # print(a,b)\n",
    "        return a, b\n",
    "\n",
    "    @property\n",
    "    def values(self):\n",
    "        quantiles = []\n",
    "        for i in range(self.k):\n",
    "            a, b = self.compute_ab(i)\n",
    "            quantiles.append(beta_dist(a,b).ppf(1-1/\n",
    "                            (self.t*np.log(self.n)**self.c)))\n",
    "        return quantiles\n",
    "\n",
    "    @property\n",
    "    def theta(self):\n",
    "        thetas = []\n",
    "        for i in range(self.k):\n",
    "            a, b = self.compute_ab(i)\n",
    "            thetas.append(a/(a+b))\n",
    "        return thetas\n",
    "\n",
    "    def action(self):\n",
    "        self.t += 1\n",
    "        return np.argmax(self.values)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.post_param[action] += reward\n",
    "        self.draws[action] += 1\n",
    "\n",
    "class HierarchicalBayesUCB(BayesUCB):    \n",
    "    @property\n",
    "    def prior_param(self):\n",
    "        u = np.mean(self.post_param/self.draws)\n",
    "        v = np.var(self.post_param/self.draws)\n",
    "        u = u if u != 1 else u+1e-10\n",
    "        v = v if v != 0 else 1e-10\n",
    "        assert v > 0, 'Variance is ' + str(v)\n",
    "        \n",
    "        b = u*(1-u)**2/v - (1-u)\n",
    "        a = u*b/(1-u)\n",
    "\n",
    "        eps = 1e-10\n",
    "        if a <= eps: a = eps\n",
    "        if b <= eps: b = eps\n",
    "\n",
    "        assert b > 0, 'b = {}, u {}, v {}'.format(b, u, v)\n",
    "        assert a > 0, 'a = {}, u {}, v {}'.format(a, u, v)\n",
    "        return [a, b]\n",
    "\n",
    "    def action(self):\n",
    "        self.t += 1\n",
    "        if self.t <= self.k:\n",
    "            # first pick each arm to avoid zero variance\n",
    "            return (self.t-1)\n",
    "        return np.argmax(self.values)\n",
    "\n",
    "class ThompsonSampling(BayesUCB):\n",
    "    @property\n",
    "    def values(self):\n",
    "        samples = []\n",
    "        for i in range(self.k):\n",
    "            a, b = self.compute_ab(i)\n",
    "            samples.append(beta_dist(a,b).rvs())\n",
    "        return samples\n",
    "\n",
    "class HierarchicalThompsonSampling(HierarchicalBayesUCB, ThompsonSampling):\n",
    "    # Watch out for deadly diamond of death inheritance.\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
